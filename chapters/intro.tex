\chapter{Introductory pills}

\section{Basic ideas}

	Note: "pick" and "sample" are used interchangeably.

	About $E(X)$: It is called the "expected value" because it means the midpoint where all outcomes, compounded with their weight, contribute to the equilibrium.
	
	\textit{Question: Can we still talk about expected value when the outcomes are not numeric? What could be a good bijection between a generic sample space $\Omega$ and $\nonneg$?}
	
\subsection{Sampling}

	\textit{Sampling} is a simple algorithmic approach used for abstracting and estimating the percentage of elements of a certain type in a large set.
	
	\ex If there is a huge number of black and white marbles in a (very big) bowl and you'd like to know the ratio of black marbles, you can't count all of them, so you extract a sample of marbles many times, until you have sufficient confidence in the obtained result.


\subsection{Union bound}
	
	\begin{equation}
	\Pr(\bigcup_{i=0}^n E_i) \leq \sum_{i=0}^{n}(\Pr(E_i))
	\end{equation}
	If the events are mutually disjoint, then the equality is strict.

	
\subsection{Markov inequality}
	
	Let $X$ be a non-negative random variable (RV) in a sample space $\Omega$, and $a$ an outcome in $\Omega$, then:
	
	\begin{equation}
	\Pr(X \geq a) \leq \frac{E[X]}{a}
	\end{equation}
	
	Proof:
	\begin{align*}
	E(X) &= \int_{0}^{\infty}x \Pr(X=x)dx \\
		 &\geq \int_{a}^{\infty}x\Pr(X=x)dx \\
		 &\geq \int_{a}^{\infty}a\Pr(X=x)dx \\
		 &= a\Pr(X \geq a)
	\end{align*}


\subsection{Moment generating functions}
	
	Moments, in a mathematical-analytical sense, are "quantitative measures" that describe characteristic of a shape. E.g.: a generic function's first moment is its slope (first derivative).\\
	Given a random variable $X$, then its moment generating function is
	\begin{equation}
	M_X(t) = e^{tX}, t \in \mathbb{R}
	\end{equation}
	
	To be continued... % TODO moment generating function
	
	Reminder! Power series of the $e^x$ function: $\displaystyle e^x = \sum_{i=0}^{\infty}\frac{x^n}{n!}$
	
	
\section{Chernoff bound}

	The Chernoff bound allows you to control how much a RV is close to its expected value.

	Let $X_1, \dots, X_n \in \coin(p) \iid$, i.e. $X_1, \dots, X_n$ are $n$ RV such that\\
	$\Pr(x_i=1)=p$ and $\Pr(x_i=0)=1-p$.\\
	In other words, they are $n$ independent coin throws of the same coin.
	
	Let $X=\sum_{i=1}^{n}(X_i)$.
	X is essentially a binomial RV: $X \in \binomdist(\binary^n, p)$, thus $E(X)=p\cdot n$.\\
	It represents the number of times you get head with the aforesaid $n$ throws.
	
	So, given an error limit $\varepsilon$, we define the Chernoff bound as:
	\begin{equation}\label{chernoff}   %True definition
	\Pr(\abs{X-E[X]} \geq t)\leq e^{-2t^2/n}
	\end{equation}
	where $t := \varepsilon n$.
	
	In the binomial case, [\ref{chernoff}] translates to:
	\begin{equation*}
	\Pr\left(\abs{\sum_{i=1}^{n}(X_i) - p \cdot n} > t \right) = \Pr\left(\abs{\frac{1}{n}\sum_{i=1}^{n}(X_i) - p} > \frac{t}{n} \right)
	\end{equation*}
	\\
	Then, replacing $t$ with $\varepsilon n$, we get %Custom
	\begin{equation}
		\Pr\left(\abs{\frac{1}{n}\sum_{i=1}^{n}(X_i) - p} > \varepsilon \right) \leq 2e^{-2n^3\varepsilon^2}
	\end{equation} % TODO check binomial c.b.
	
	\ex If you want to know how many Facebook users have more than 100 friends, you can't simply count, but you can obtain a good approximation sampling a reasonable number of profiles.
	
	It's important to observe that the size of the sample we need in order to obtain a good approximation depends on the error we tolerate, not on the total population.
	
	There are many possible formulations of the Chernoff bound, another one that can be useful is the following:
	\begin{equation}\label{chernoff2}
	\Pr(\abs{X-E[X]} \geq \varepsilon \cdot n) \leq 2e^{-\frac{\varepsilon^{2}}{3}n}
	\end{equation}
	
	\ex Let be $E[X]=\frac{50}{100}$ and $\varepsilon=\frac{1}{100}$, the probability that $X \geq \frac{51}{100}$ or $X \geq \frac{49}{100}$ is less then or equal to $2e^{-\frac{1}{30000}n}$.