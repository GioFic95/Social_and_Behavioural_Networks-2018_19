% !TEX spellcheck = en-US

\section{More distances}
	Let $A \vartriangle B = (A-B) \cup (B-A)$.

	Dice similarity:
	\begin{equation}
	\displaystyle D(A, B) = \frac{|A\cap B|}{|A\cap B| + \frac{1}{2}|A\vartriangle B|}
	\end{equation}
	
	Anderberg similarity:
	\begin{equation}
	\displaystyle An(A, B) = \frac{|A\cap B|}{|A\cap B| + 2|A\vartriangle B|}
	\end{equation}
	
	Generalizing Jaccard, Dice and Anderberg: 
	\begin{equation}
	\displaystyle S_\gamma(A, B) = \frac{|A\cap B|}{|A\cap B| + \gamma|A\vartriangle B|}
	\end{equation}
	By this third definition we can obtain $\jaccsim$ with $\gamma=1$, $D$ with $\gamma=\frac{1}{2}$, and $An$ with $\gamma=2$.
	
	\textit{Question}: for which values of $\gamma$ does an LSH exist for $S$? The next lemma helps us finding an answer.
	
	\lem (by Charikar) If a similarity $S$ admits an LSH, then the function $1-S$ must satisfy the triangular inequality, i.e. $\forall A,B,C \in U$ \\ $d(A,B) \leq d(A,C) + d(B,C)$, where the distance $d(A,B) = 1-S(A,B)$ is our function $1-S$.
	
	\newpage
	Proof: Let $E_{XY}$ be the event "$h(X) \neq h(Y)$"; since $S$ admits a LSH, we have:
	\begin{align*}
		d(A,B) &= 1-S(A,B) = \Pr_h(E_{AB}) = p_1 + p_2 + p_3 + p_4 \\
		d(A,C) &= 1-S(A,C) = \Pr_h(E_{AC}) = p_1 + p_3 + p_5 + p_7 \\
		d(B,C) &= 1-S(B,C) = \Pr_h(E_{BC}) = p_1 + p_2 + p_5 + p_6
	\end{align*}
	with the probabilities $p_i$ defined as in the following table, where an $X$ under the "may exist" column means that the corresponding probability is $0$ (it happens because transitivity generates contradictions, see example [\ref{ex:transitivity}]): \\\\
	\begin{tabular}{lllll}
		& $E_{AB}$ & $E_{AC}$ & $E_{BC}$ & may exist             \\
		$p_1$ & T         & T         & T         & $\checkmark$ \\
		$p_2$ & T         & T         & F         & $\checkmark$ \\
		$p_3$ & T         & F         & T         & $\checkmark$ \\
		$p_4$ & T         & F         & F         & X            \\
		$p_5$ & F         & T         & T         & $\checkmark$ \\
		$p_6$ & F         & T         & F         & X            \\
		$p_7$ & F         & F         & T         & X            \\
		$p_8$ & F         & F         & F         & $\checkmark$
	\end{tabular} \\\\
	Hence we have
	\begin{equation*}
		d(A,C) + d(B,C) = 2p_1 + p_2 + p_3 + 2p_5 \geq p_1 + p_2 + p_3 = d(A,B)
	\end{equation*}
	and so $1-S$ doesn't comply with the triangular inequality, QED.
	
	\obs Similarities are actually defined in most cases as the inverses of measures, which in turn give (oh so surprisingly!) a notion of distance.
	
	\cor By Charikar's lemma, we can prove that Dice's similarity cannot admit a LSH scheme.
	
	\textit{Proof} by counterexample: Assume $A=\{1\}, B=\{2\}, C=\{1, 2\}$, then use the triangular inequality over the distances: \\
	$D(A,C)=\frac{2}{3},\ D(B,C)=\frac{2}{3},\ D(A,B)=0$
	\begin{multline*}
		d(A,B) = 1- D(A,B) = 1 > \\
		\frac{2}{3} = (1-D(A,C)) + (1-D(B,C)) = d(A,C) + d(B,C)
	\end{multline*}
	hence it doesn't comply with the triangular inequality, QED.\\
	(Note that $D$ stands for Dice similarity and $d$ for distance)
	
	\obs Parameterizing this counterexample with $S_\gamma$, we obtain a bounds for $\gamma$: let $A=\{1\}, B=\{2\}, C=\{1, 2\}$ and \\
	$S_\gamma(A,C)=\frac{1}{1+\gamma},\ S_\gamma(B,C)=\frac{1}{1+\gamma},\ S_\gamma(A,B)=0$, thus
	\begin{multline*}
		1=1-S_\gamma(A,B)=d(A,B) > d(A,C) + d(B,C) = \\
		(1-S_\gamma(A,C)) + (1-S_\gamma(B,C)) = 2 \left( 1- \frac{1}{1+\gamma} \right) = \frac{2\gamma}{1 + \gamma}
	\end{multline*}
	from which we obtain
	$1 > \frac{2\gamma}{1 + \gamma} \Rightarrow \gamma < 1$. \\
	Hence, if $\gamma < 1$, the triangular inequality doesn't hold, so no LSH can exist, as in the case of the Dice similarity.
	
\section{Probability generating functions}
	
	\textit{Intuition}: A probability generating function (PGF) is a power series representation of a given probability distribution
	
	\textbf{Definition}: Given a (discrete) RV $X$, its \textbf{PGF} is the function:
	\begin{equation}
		\mathcal{G}en_X(\alpha)= \sum_{x=0}^{\infty}\Pr(X=x)\alpha^x
	\end{equation}
	(note that all outcomes appear by their probability).
	
	How to get back to pmf: $\displaystyle \Pr(X=x) = \frac{\mathcal{D}^x(\mathcal{G}en_X(0))}{x!}$  % TODO chi Ã¨ D? e pmf?
	
	In other terms, a PGF $f$ is a function:
	\begin{equation}
		f(x)= \sum_{x=0}^{\infty}\left( p_i x^i \right)
	\end{equation}
	s.t. $p_i\geq 0 \forall i$ and $\sum_{x=0}^{\infty} p_i = 1$.
	
	\thm: If a similarity $S$ admits a LSH and a given function $f$ is a PGF, then $f(S)$ admits a LSH.
	\begin{equation*}
		f(S(A,B))=(f(S)) (A,B) =: T(A,B)
	\end{equation*}
	
	% TODO check
	% Afterthought: Are we "applying" a probability distribution over a similarity (which in turn, since it is lshable, means it has a probdist over a subset of hashfunctions)?
	
	Before going into the proof of the theorem we show a consequence of it.
	
	\obs Applying the theorem to the Jaccard similarity: \\
	Our function is $f_\gamma$, with $\gamma > 1$, defined as
	\begin{equation} \label{eq:pgf_jacc}
		\displaystyle f_\gamma(x) = \frac{x}{x+\gamma(1-x)}
	\end{equation} \\
	In order to proof $f_\gamma$ is a PGF, we have to demonstrate that the coefficients represent a probability distribution: i.e., they are all positive and they sum to zero. \\
	By applying the Taylor series expansion to [\ref{eq:pgf_jacc}], we get \\
	$\displaystyle
		f_\gamma(x) = \sum_{x=1}^{\infty}\left(\frac{\left(1-\frac{1}{\gamma}\right)^i}{\gamma -1}x^i\right)
	$; now $f_\gamma$ is a power series, so all the coefficients are positive. \\
	In order for the sum of the coefficients to be equal to 1, the numerator must be equal to the denominator: \\
	$\displaystyle \sum_{i=1}^{\infty} \left(\frac{\left(1-\frac{1}{\gamma}\right)^i}{\gamma -1} \right) = 1 \Rightarrow \sum_{i=1}^{\infty} \left( \left(1-\frac{1}{\gamma}\right)^i \right) = \gamma -1$. \\
	Indeed we have:
	\begin{flalign*}
		\sum_{i=1}^{\infty}\left(1 - \frac{1}{\gamma}\right)^i
		&= \left(1 - \frac{1}{\gamma}\right)\sum_{i=0}^{\infty}\left(1 - \frac{1}{\gamma}\right)^i &&\\
		&=^{\left(*\right)} \left(1 - \frac{1}{\gamma}\right) \frac{1}{1 - \left(1 - \frac{1}{\gamma}\right)} &&\\
		&= \frac{\gamma -1 }{\gamma}\frac{1}{1/\gamma} = \gamma -1 &&
	\end{flalign*}
	where the step marked with $\left(*\right)$ is due to the equality $\sum_{i=0}^{\infty} \alpha^i = \frac{1}{1-\alpha}$. \\
	And with this we proved that $f_\gamma$ is a PGF.
	
	Now we can apply PGF to a similarity $S_\gamma$:
	\begin{align*}
	f_\gamma (S_\gamma(A, B))
	&= f_\gamma \left( \frac{\abs{A \cap B}}{\abs{A \cup B}} \right) \\
	&= \frac{\frac{\abs{A \cap B}}{\abs{A \cup B}}}
		{\frac{\abs{A \cap B}}{\abs{A \cup B}} +
			\gamma \frac{\abs{A \cup B} -
				\abs{A \cap B}}{\abs{A \cup B}}} \\
	&= \frac{\abs{A \cap B}}
		{\abs{A \cap B} + \gamma \abs{A \vartriangle B}}
	 = S_\gamma(A, B)
	\end{align*}
	which in turn is LSH-able
	
\section{181008}  % TODO la lasciamo questa sezione?
	
	%% So it actually matches the previous wikipedian def
	\textit{Recap}: Given a universe $U$, a function $S \in U^2 \to [0, 1]$ is said to be a \textbf{LSHable similarity} iff exists a prob. distr. over (a family/subset of) the hash functions in $U$, such that: 
	\begin{equation}
	\forall \{X, Y\} \in \binom{U}{2}\ \ \Pr_h(h(X)=h(Y)) = S(X, Y)
	\end{equation}
	
	\thm If a similarity $S$ is LSH-able and $f$ is a PGF, then $f(S)$ is LSHable.
	
	\textit{Equivalent statement}:
	\begin{equation}
	f(S) := T \in U^2 \to [0, 1] \text{ s.t. } \forall \{A, B\} \in \binom{U}{2}\ \ T(A, B) = f(S(A, B))
	\end{equation}
	
	% TODO check
	%Afterthought: could we just demonstrate that probability distributions are composable? lshability could be just a carried-over property...
	
	Lemma (1): The similarity $O \in U^2 \to [0, 1] : (A, B) \mapsto 1$ admits a LSH
	Proof: Give prob. 1 to a constant function (duh!): $h \in U \to \mathbbm{1} A \mapsto 0 \forall A in U$
	
	Purpose: This will be the base case for theorem proof...
	
	%%%%%%%%
	
	\
	
	Lemma(2): If S and T similarities over U have a scheme, then $S \cdot T : (S \cdot T)(A, B) = S(A, B)\cdot T(A, B)$ has a scheme
	
	(I.E:). LSHability is preserved upon composition/multiplication
	
	Proof by construction (Algorithm): 
	
	sample hash functions for $S \cdot T$ as follows
	
	first, sample $h_S$ for S;
	then sample $h_T$ independently for T
	return the function $h : A \mapsto (h_S(A), h_T(A))$
	
	$\Pr_h(h(A)=h(B)) = \Pr_{h_S}(h_S(A)=h_S(B)) \cdot \Pr_{h_T}(h_T(A)=h_T(B)) = S(A, B) \cdot T(A, B) (\forall \{A, B\} \in \mathcal{P}_2(U))$by independency
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	Lemma(3): if S is LSHable then $\forall i \in \mathbb{N}$ $S^i$ is lshable
	
	proof by induction:
	
	base (Lemma1): i=0 and $S^0$=O OK
	
	ind: Use lemma 2 on $S^i$ and $S$ to obtain $S^{i+1}$; S has a scheme by def, $S^i$ has a scheme by induction hypothesis
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	lemma(4): if $p_0, ..., p_i, ... : \sum_{i=0}^{\infty}p_i=1 , and p_i\geq 0 \forall i$, and $S_0, ..., S_i, ...$ are lshable similarities, then $\sum_{i=0}^{\infty}p_iS_i$ is lshable
	
	scheme: first pick(sample, they are synonyms) i* at random from $\mathbb{N}$ with probability $p_0, ..., p_i, ...$
	then, sample h from the hash functions of $S_{i*}$
	
	$\Pr(h(A)=h(B))=\sum_{i=0}^{\infty}(p_i S_i(A, B))$
	
	$\Pr(h(A)=h(B))=\sum_{i=0}^{\infty}(\Pr(i=i*)\Pr_h(h(A)=h(B) | i=i*))$, $\Pr(i=i*) = p_i$, $hahb|i=i* \to S_i(A, B)$
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	Final proof: sum of $p_iS^i$ has a scheme
	
	L3: $S^i$ has a scheme
	
	L4: the sum is lshable, proven
	
	%%%%%%%%%%%%%%%%%%%%%%%
	
	Example of a pgf: $sum(2^-1 times x^i)$
	
	
	----------------------------------------------------------------??sorensen dice??cosine similarity?? inner product??johnson-lindenstrauss??
	
	a sketch is n instance of a pgf which can be used to implement other similarities NONONONONOO
	
	
	%%%%%%%%%%%J
	
	PGF is an approach for making schemes for similarities from other schemes
	
\section{181010}
	
	Let f a PGF, $\alpha \in [0, 1]$, ...?
	
	%&
	$\alpha f = \alpha f(S)\ |\ (1 - \alpha)T$
	%&
	
	$T \in U^2 \to [0, 1] : \forall {t, t'} \in \mathcal{P}_2(U) T(t, t') = 0$
	
	for the 1 case we wanted a nbanal partition, now with 0 we want a punctual partition, so we need a hash function that assigns a distinct value to each argument. (You can actually use the identity)
	
	Not a good scheme, because we're not shrinking data
	
	GOTO %&
	
\subsubsection{Approximation examples}
	
	Consider $S_\gamma$: $\gamma \geq 1 \Leftrightarrow S_\gamma$ is LSH-able
	
	Focus on $\gamma < 1$
	
	Definition: "Distortion of a similarity"
	
	Let S be a similarity, then its distortion is "the minimum*(meaning inferior extremum) delta geq 1 : exists LSHABLE S' (forall {A, B} in P2U, $1/\delta \cdot S(A, B) \leq S'(A, B) \leq S(A, B)$)"
	
	delta tends to 1 -> S is lshable
	
	using jaccard to approximate sgamma we wold obtain 1/gamma
	
	delta is the approximation factor
	
	%%%%%%%%%%%%%%%%%%
	
	Centerlemma: Let S be a LSHable similarity $ : \exists \chi \subseteq U : \forall \{x, x'\} \in \mathcal{P}_2(\chi) S(x, x')=0$, then
	
	$\forall y \in U avg_{x \in \chi}(S(X, Y) \leq) \frac{1}{|\chi|}$; (it trivially follows that $\exists x* \in \chi : S(x*, Y \leq \frac{1}{|\chi|})$)
	
	Proof: (Fix $y \in U$) If the hash function h has positive probability in the (chosen) lsh for S, then $\forall {x, x'} \in \mathcal{P}_2(\chi) (h(x)\neq h(x'))$
	
	[$\chi$ is actually a (possibly incomplete) section of the partition of U induced by h]
	
	thus, forall hash functions with positive probability, there can exist at most one x in calx st hx=hy (transitivity of equality)
	
	$\sum_{x \in \chi}S(x, y) = \sum_{x \in \chi}\Pr_h(h(x)=h(y)) = \sum_{x \in \chi}\sum_{h}\Pr(h\ is\ chosen)[h(x)=h(y)]$
	
	IMPORTANT: The brackets here are a boolean evaluation operator
	
	$= \sum pr h is chosen \sum eval hx = hy \leq \sum_h \Pr(h\ is\ chosen) \leq 1$
	
	Thus, $\sum_{x \in \chi}S(x, y) \leq 1 \Rightarrow avg(S(X, Y)) \leq \frac{1}{|\chi|}$, proven
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	$S:= S_\gamma , 0 < \gamma < 1, U=2^{[n]}={S|S\subseteq [n]}$ %insieme delle parti?
	
	$\chi := \mathcal{P}_1([n])$
	$y = [n]$
	
	 - let us assume that T finitely distorts sgamma, and T is lshable
	then $T({},{}) = 0 forall {i, j} in P_2(mathbbm(n))$
	
	 - $\exists \{i\} in \chi : T(\{i\}, [n]) \leq \frac{1}{|\chi|} = \frac{1}{n}$
	 
	 $S_\gamma(\{i\}, [n]) = \frac{1}{1 + \gamma(n-1)} = \frac{1}{\gamma n + (1-\gamma)}$
	 
	 ...
	 zenterlemma application
	 ...
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	$ U = {}$