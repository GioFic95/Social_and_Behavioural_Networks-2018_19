% !TEX spellcheck = en-US

\section{More distances}
	Let $A \vartriangle B = (A-B) \cup (B-A)$.

	Dice similarity:
	\begin{equation}
	\displaystyle D(A, B) = \frac{|A\cap B|}{|A\cap B| + \frac{1}{2}|A\vartriangle B|}
	\end{equation}
	
	Anderberg similarity:
	\begin{equation}
	\displaystyle An(A, B) = \frac{|A\cap B|}{|A\cap B| + 2|A\vartriangle B|}
	\end{equation}
	
	Generalizing Jaccard, Dice and Anderberg: 
	\begin{equation}
	\displaystyle S_\gamma(A, B) = \frac{|A\cap B|}{|A\cap B| + \gamma|A\vartriangle B|}
	\end{equation}
	By this third definition we can obtain $\jaccsim$ with $\gamma=1$, $D$ with $\gamma=\frac{1}{2}$, and $An$ with $\gamma=2$.
	
	\textit{Question}: for which values of $\gamma$ does an LSH exist for $S$? The next lemma helps us finding an answer.
	
	\lem (by Charikar) If a similarity $S$ admits an LSH, then the function $1-S$ must satisfy the triangular inequality, i.e. $\forall A,B,C \in U$ \\ $d(A,B) \leq d(A,C) + d(B,C)$, where the distance $d(A,B) = 1-S(A,B)$ is our function $1-S$.
	
	\newpage
	Proof: Let $E_{XY}$ be the event "$h(X) \neq h(Y)$"; since $S$ admits a LSH, we have:
	\begin{align*}
		d(A,B) &= 1-S(A,B) = \Pr_h(E_{AB}) = p_1 + p_2 + p_3 + p_4 \\
		d(A,C) &= 1-S(A,C) = \Pr_h(E_{AC}) = p_1 + p_3 + p_5 + p_7 \\
		d(B,C) &= 1-S(B,C) = \Pr_h(E_{BC}) = p_1 + p_2 + p_5 + p_6
	\end{align*}
	with the probabilities $p_i$ defined as in the following table, where an $X$ under the "may exist" column means that the corresponding probability is $0$ (it happens because transitivity generates contradictions, see example [\ref{ex:transitivity}]):

	\begin{tabular}{lllll}
		& $E_{AB}$ & $E_{AC}$ & $E_{BC}$ & may exist             \\
		$p_1$ & T         & T         & T         & $\checkmark$ \\
		$p_2$ & T         & T         & F         & $\checkmark$ \\
		$p_3$ & T         & F         & T         & $\checkmark$ \\
		$p_4$ & T         & F         & F         & X            \\
		$p_5$ & F         & T         & T         & $\checkmark$ \\
		$p_6$ & F         & T         & F         & X            \\
		$p_7$ & F         & F         & T         & X            \\
		$p_8$ & F         & F         & F         & $\checkmark$
	\end{tabular}

	\noindent Hence we have
	\begin{equation*}
		d(A,C) + d(B,C) = 2p_1 + p_2 + p_3 + 2p_5 \geq p_1 + p_2 + p_3 = d(A,B)
	\end{equation*}
	and so $1-S$ doesn't comply with the triangular inequality, QED.
	
	\obs Similarities are actually defined in most cases as the inverses of measures, which in turn give (oh so surprisingly!) a notion of distance.
	
	\cor By Charikar's lemma, we can prove that Dice's similarity cannot admit a LSH scheme.
	
	\textit{Proof} by counterexample: Assume $A=\{1\}, B=\{2\}, C=\{1, 2\}$, then use the triangular inequality over the distances: \\
	$D(A,C)=\frac{2}{3},\ D(B,C)=\frac{2}{3},\ D(A,B)=0$
	\begin{multline*}
		d(A,B) = 1- D(A,B) = 1 > \\
		\frac{2}{3} = (1-D(A,C)) + (1-D(B,C)) = d(A,C) + d(B,C)
	\end{multline*}
	hence it doesn't comply with the triangular inequality, QED.\\
	(Note that $D$ stands for Dice similarity and $d$ for distance)
	
	\obs Parameterizing this counterexample with $S_\gamma$, we obtain a bounds for $\gamma$: let $A=\{1\}, B=\{2\}, C=\{1, 2\}$ and \\
	$S_\gamma(A,C)=\frac{1}{1+\gamma},\ S_\gamma(B,C)=\frac{1}{1+\gamma},\ S_\gamma(A,B)=0$, thus
	\begin{multline*}
		1=1-S_\gamma(A,B)=d(A,B) > d(A,C) + d(B,C) = \\
		(1-S_\gamma(A,C)) + (1-S_\gamma(B,C)) = 2 \left( 1- \frac{1}{1+\gamma} \right) = \frac{2\gamma}{1 + \gamma}
	\end{multline*}
	from which we obtain
	$1 > \frac{2\gamma}{1 + \gamma} \Rightarrow \gamma < 1$. \\
	Hence, if $\gamma < 1$, the triangular inequality doesn't hold, so no LSH can exist, as in the case of the Dice similarity.
	
\section{Probability generating functions}
	
	\textit{Intuition}: A probability generating function (PGF) is a power series representation of a given probability distribution
	
	\textbf{Definition}: Given a (discrete) RV $X$, its \textbf{PGF} is the function:
	\begin{equation}
		\mathcal{G}en_X(\alpha)= \sum_{x=0}^{\infty}\Pr(X=x)\alpha^x
	\end{equation}
	(note that all outcomes appear by their probability).
	
	How to get back to pmf: $\displaystyle \Pr(X=x) = \frac{\mathcal{D}^x(\mathcal{G}en_X(0))}{x!}$  % TODO chi è D? e pmf?
	
	In other terms, a PGF $f$ is a function:
	\begin{equation}
		f(x)= \sum_{x=0}^{\infty}\left( p_i x^i \right)
	\end{equation}
	s.t. $p_i\geq 0 \forall i$ and $\sum_{x=0}^{\infty} p_i = 1$.
	
	\thm: If a similarity $S$ admits a LSH and a given function $f$ is a PGF, then $f(S)$ admits a LSH.
	\begin{equation*}
		f(S(A,B))=(f(S)) (A,B) =: T(A,B)
	\end{equation*}
	
	% TODO check
	% Afterthought: Are we "applying" a probability distribution over a similarity (which in turn, since it is lshable, means it has a probdist over a subset of hashfunctions)?
	
	Before going into the proof of the theorem we show a consequence of it.
	
	\obs Applying the theorem to the Jaccard similarity: \\
	Our function is $f_\gamma$, with $\gamma > 1$, defined as
	\begin{equation} \label{eq:pgf_jacc}
		\displaystyle f_\gamma(x) = \frac{x}{x+\gamma(1-x)}
	\end{equation} \\
	In order to proof $f_\gamma$ is a PGF, we have to demonstrate that the coefficients represent a probability distribution: i.e., they are all positive and they sum to zero. \\
	By applying the Taylor series expansion to [\ref{eq:pgf_jacc}], we get \\
	$\displaystyle
		f_\gamma(x) = \sum_{x=1}^{\infty}\left(\frac{\left(1-\frac{1}{\gamma}\right)^i}{\gamma -1}x^i\right)
	$; now $f_\gamma$ is a power series, so all the coefficients are positive. \\
	In order for the sum of the coefficients to be equal to 1, the numerator must be equal to the denominator: \\
	$\displaystyle \sum_{i=1}^{\infty} \left(\frac{\left(1-\frac{1}{\gamma}\right)^i}{\gamma -1} \right) = 1 \Rightarrow \sum_{i=1}^{\infty} \left( \left(1-\frac{1}{\gamma}\right)^i \right) = \gamma -1$. \\
	Indeed we have:
	\begin{flalign*}
		\sum_{i=1}^{\infty}\left(1 - \frac{1}{\gamma}\right)^i
		&= \left(1 - \frac{1}{\gamma}\right)\sum_{i=0}^{\infty}\left(1 - \frac{1}{\gamma}\right)^i &&\\
		&=^{\left(*\right)} \left(1 - \frac{1}{\gamma}\right) \frac{1}{1 - \left(1 - \frac{1}{\gamma}\right)} &&\\
		&= \frac{\gamma -1 }{\gamma}\frac{1}{1/\gamma} = \gamma -1 &&
	\end{flalign*}
	where the step marked with $\left(*\right)$ is due to the equality $\sum_{i=0}^{\infty} \alpha^i = \frac{1}{1-\alpha}$. \\
	And with this we proved that $f_\gamma$ is a PGF.
	
	Now we can apply PGF to a similarity $S_\gamma$:
	\begin{align*}
	f_\gamma (S_\gamma(A, B))
	&= f_\gamma \left( \frac{\abs{A \cap B}}{\abs{A \cup B}} \right) \\
	&= \frac{\frac{\abs{A \cap B}}{\abs{A \cup B}}}
		{\frac{\abs{A \cap B}}{\abs{A \cup B}} +
			\gamma \frac{\abs{A \cup B} -
				\abs{A \cap B}}{\abs{A \cup B}}} \\
	&= \frac{\abs{A \cap B}}
		{\abs{A \cap B} + \gamma \abs{A \vartriangle B}}
	 = S_\gamma(A, B)
	\end{align*}
	which in turn is LSH-able
	
\section{181008}  % TODO la lasciamo questa sezione?
	
	%% So it actually matches the previous wikipedian def
	\textit{Recap}: Given a universe $U$, a function $S \in U^2 \to [0, 1]$ is said to be a \textbf{LSHable similarity} iff exists a prob. distr. over (a family/subset of) the hash functions in $U$, such that: 
	\begin{equation}
	\forall \{X, Y\} \in \binom{U}{2}\ \ \Pr_h(h(X)=h(Y)) = S(X, Y)
	\end{equation}
	
	\thm \label{t:pgf_1} If a similarity $S$ is LSH-able and $f$ is a PGF, then $f(S)$ is LSHable.
	
	\textit{Equivalent statement}:
	\begin{equation}
	f(S) := T \in U^2 \to [0, 1] \text{ s.t. } \forall \{A, B\} \in \binom{U}{2}\ \ T(A, B) = f(S(A, B))
	\end{equation}
	
	% TODO check
	%Afterthought: could we just demonstrate that probability distributions are composable? lshability could be just a carried-over property...
	
	\lem \label{l:pgf_1} (L1) The similarity $O \in U^2 \to [0, 1]$ s.t. $O(A, B) \mapsto 1 \ \forall \{A, B\} \in \binom{U}{2}$ admits a LSH.
	
	\textit{Proof}: Give probability 1 to the constant hash function $h$: $h(A)=1 \ \forall A\in U$\\
	$\Pr(h(A)=h(B))=1=O(A,B) \ \forall \{A, B\} \in \binom{U}{2}$.
	
	% TODO mi sembra più chiaro così ^
	% Give prob. 1 to a constant function (duh!): $h \in U \to \mathbbm{1} A \mapsto 0 \forall A in U$
	
	\textit{Purpose}: This will be the base case for theorem proof...
	
	\lem \label{l:pgf_2} (L2) If $S$ and $T$ similarities over $U$ have a scheme, then $S \cdot T : (S \cdot T)(A, B) = S(A, B)\cdot T(A, B)$ has a scheme. \\
	I.e. LSHability is preserved upon composition/multiplication.
	
	\textit{Proof} by construction (Algorithm): 
	\begin{itemize}
		\item Sample hash functions for $S \cdot T$ as follows:
		\begin{enumerate}
			\item first, sample $h_S$ from the LSH for S;
			\item then, sample $h_T$ independently from the LSH for T;
		\end{enumerate}
		\item Return the hash function $h$ s.t.
		$H(A) = (h_S(A), h_T(A)) \ \forall A \in U$;
		\item Thus, $\forall \{A, B\} \in \binom{U}{2}$, we have:
		\begin{flalign*}
			\Pr_h(h(A)=h(B))
			&= \Pr_{h_S}(h_S(A)=h_S(B)) \cdot \Pr_{h_T}(h_T(A)=h_T(B)) &&\\
			&= S(A, B) \cdot T(A, B) &&
		\end{flalign*}
		by independency.
	\end{itemize}
	
	\lem \label{l:pgf_3} (L3) if $S$ is LSHable, then $S^i$ is LSHable, $\forall i \in \mathbb{N}$.
	
	\
	
	\textit{Proof} by induction:
	\begin{itemize}
		\item Base: $i=0$ and $S^0=O$ \\
			True for L1 [\ref{l:pgf_1}];
		\item Inductive hypothesis: $S^i$ is LSHable;
		\item Inductive step: $S^{i+1}$ is LSHable \\
			True because $S^{i+1}= S \cdot S^i$ is LSHable by L2 [\ref{l:pgf_2}], since $S$ has a scheme by def and $S^i$ has a scheme by inductive hypothesis.
	\end{itemize}
	
	\lem \label{l:pgf_4} (4) If $p_0, p_1, ..., p_i, ...$ is such that $\sum_{i=0}^{\infty}p_i=1$, $p_i\geq 0 \ \forall i$, and $S_0, S_1, ... , S_i, ...$ are lshable similarities, then $\sum_{i=0}^{\infty}p_iS_i$ is lshable.
	
	\textit{Proof} by scheme:
	\begin{enumerate}
	\item First, sample $i*$ at random from $\mathbb{N}$ with probability $p_0, ..., p_i, ...$;
	\item Then, sample $h$ from the hash functions (LSH) of $S_{i*}$ (that is, we are choosing which LSH to use);
	\item Thus we have:
	\begin{flalign*}
		\Pr_h(h(A)=h(B))
		&=\sum_{i=0}^{\infty}(p_i S_i(A, B)) &&\\
		&=\sum_{i=0}^{\infty}(\underbrace{\Pr(i=i*)}_{p_i} \cdot \underbrace{\Pr_h(h(A)=h(B) | i=i*))}_{S_i(A,B)} &&\\
	\end{flalign*}
	\end{enumerate}

	\obs This lemma is useful if we need a weighted average \\
	$W(A,B) = \sum_{i=0}^{\infty}(p_i S_i(A, B))$.
	
	\textit{Proof} of the theorem [\ref{t:pgf_1}]:
	\begin{itemize}
	\item We want to prove that $\sum_{i=0}^{\infty}p_iS^i$ has a scheme (is LSHable);
	\item By L3 [\ref{l:pgf_3}] we know $S^i$ has a scheme;
	\item By L4 [\ref{l:pgf_4}] we know the sum is lshable;
	\item Proven.
	\end{itemize}
	
	\obs This theorem tells us how to build a LSH: by concatenating the results of different hash functions, keeping the output small. \\ I.e. PGF is an approach for making schemes for similarities from other schemes.
	\ex $\sum_{i=1}^{a}(2^{-i} \cdot x^i)$.
	
	\section{A mention of the sketches}
	
	Like LSH, they're a means for simplify the storage of data; they are slower than LSH but allow you to keep interesting information in a small space.
	
	\textbf{Definition}: a sketch is a representation of big objects with small images. They can be used to retrieve interesting information about original objects without regain the whole original objects (i.e. they are not compression algorithms).
	
	\ex It's possible to go back to $\abs{A \cap B}$ from only $|A|, |B|, h(A), h(B),$ $\jaccsim(A,B)\pm\varepsilon$. Note that $\jaccsim(A,B)\pm\varepsilon$ can be obtained from $h(A), h(B)$ by applying the Chernoff Bounf [\ref{chernoff}] and the other data need only few bits to be stored.
	
	\ex It's possible to approximate $\frac{\abs{A \cap B}}{\abs{A \cap B} + \frac{1}{2}\abs{A \vartriangle B}}$ with \\
	 $\frac{1}{2} \cdot \abs{A \cup B} + \frac{1}{2} \cdot \abs{A \cap B}$.
	
	% TODO check
	% --------------------------------------------------------------
	% ??sorensen dice??cosine similarity?? inner product??johnson-lindenstrauss??
	
	%a sketch is n instance of a pgf which can be used to implement other similarities NONONONONOO
	
	
	
\section{Approximation of non-LSHable functions}

	What can we do if there is no LSH for the similarity we want/need to use? We can approximate a function without a LSH with an LSHable one.
	
	\ex We can approximate $S_\gamma$ with $\jaccsim$ with an error of $\frac{1}{\gamma}$.
	
	Let f a PGF, $\alpha \in [0, 1]$, ...?
	
	%&
	$\alpha f = \alpha f(S)\ |\ (1 - \alpha)T$
	%&
	
	$T \in U^2 \to [0, 1] : \forall {t, t'} \in \mathcal{P}_2(U) T(t, t') = 0$
	
	for the 1 case we wanted a nbanal partition, now with 0 we want a punctual partition, so we need a hash function that assigns a distinct value to each argument. (You can actually use the identity)
	
	Not a good scheme, because we're not shrinking data
	
	GOTO %&
	
\subsubsection{Approximation examples}
	
	Consider $S_\gamma$: $\gamma \geq 1 \Leftrightarrow S_\gamma$ is LSH-able
	
	Focus on $\gamma < 1$
	
	Definition: "Distortion of a similarity"
	
	Let S be a similarity, then its distortion is "the minimum*(meaning inferior extremum) delta geq 1 : exists LSHABLE S' (forall {A, B} in P2U, $1/\delta \cdot S(A, B) \leq S'(A, B) \leq S(A, B)$)"
	
	delta tends to 1 -> S is lshable
	
	using jaccard to approximate sgamma we wold obtain 1/gamma
	
	delta is the approximation factor
	
	%%%%%%%%%%%%%%%%%%
	
	Centerlemma: Let S be a LSHable similarity:
	\[
		\exists \chi \subseteq U : \forall \{x, x'\} \in \mathcal{P}_2(\chi) S(x, x')=0
	\]

	Then:
	
	$\forall y \in U avg_{x \in \chi}(S(X, Y) \leq) \frac{1}{|\chi|}$; (it trivially follows that $\exists x* \in \chi : S(x*, Y \leq \frac{1}{|\chi|})$)
	
	Proof: (Fix $y \in U$) If the hash function h has positive probability in the (chosen) lsh for S, then $\forall {x, x'} \in \mathcal{P}_2(\chi) (h(x)\neq h(x'))$
	
	[$\chi$ is actually a (possibly incomplete) section of the partition of U induced by h]
	
	thus, forall hash functions with positive probability, there can exist at most one x in calx st hx=hy (transitivity of equality)
	
	$\sum_{x \in \chi}S(x, y) = \sum_{x \in \chi}\Pr_h(h(x)=h(y)) = \sum_{x \in \chi}\sum_{h}\Pr(h\ is\ chosen)[h(x)=h(y)]$
	
	IMPORTANT: The brackets here are a boolean evaluation operator
	
	$= \sum pr h is chosen \sum eval hx = hy \leq \sum_h \Pr(h\ is\ chosen) \leq 1$
	
	Thus, $\sum_{x \in \chi}S(x, y) \leq 1 \Rightarrow avg(S(X, Y)) \leq \frac{1}{|\chi|}$, proven
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	$S:= S_\gamma , 0 < \gamma < 1, U=2^{[n]}={S|S\subseteq [n]}$ %insieme delle parti?
	
	$\chi := \mathcal{P}_1([n])$
	$y = [n]$
	
	 - let us assume that T finitely distorts sgamma, and T is lshable
	then $T({},{}) = 0 forall {i, j} in P_2(mathbbm(n))$
	
	 - $\exists \{i\} in \chi : T(\{i\}, [n]) \leq \frac{1}{|\chi|} = \frac{1}{n}$
	 
	 $S_\gamma(\{i\}, [n]) = \frac{1}{1 + \gamma(n-1)} = \frac{1}{\gamma n + (1-\gamma)}$
	 
	 ...
	 zenterlemma application
	 ...
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	$ U = {}$