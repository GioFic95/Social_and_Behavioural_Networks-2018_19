%!TEX root=../main.tex
\chapter{Expert learning}\label{sec:experts}

Expert learning is an approach we use in machine learning (or more generally in forecasting making) when we take decisions based on experts' opinion. Here \textit{experts} are black boxes that produces predictions, or guesses, such as ML models or tipsters.

\begin{ex}
    Next Sunday there will be a match between the teams $A$ and $B$, we want to know if $A$ will win the match. We read $n$ guesses on a newspaper, made by $n$ experts: $E_1$ says 1 (yes), $E_2$ says 0 (no), and so on: $E_3: 0, \ldots, E_n:1$. We want to discriminate between good predictors and random coin flips, to follow the advises of the best predictor.
\end{ex}

The general setting is the following: at time 1, we read the predictions of the experts and we make our guess, at time 2 we check if our guess was right, we look at the new predictions of the experts and we make our new guess, and so on, until time $T$; so, at each step, there are two phases: in the first we look the experts guesses, in the second we make our guess.\\
We use the following notation: the time goes from $1$ to $T$ and the current time is $t$, $G_i(t)$ is the guess made by expert $i$ at time $t$, $A(t)$ is the guess made by our algorithm at time $t$, $X(t)$ is the ground truth at time $t$ (we can know it only at time $t+1$). All of this is summarized in table [\ref{tab:experts}].
\begin{table}[h]
    \centering
    \begin{tabular}{|cccccc|}
        \hline
        $G_1(1)$ & $G_1(2)$ & $\cdots$ & $G_1(t)$ & $\cdots$ & $G_1(T)$ \\
        $G_2(1)$ & $G_2(2)$ & $\cdots$ & $G_2(t)$ & $cdots$ & $G_2(T)$ \\
        \vdots   & \vdots   & $\cdots$ & \vdots   & $\cdots$ & \vdots   \\
        $G_n(1)$ & $G_n(2)$ & $\cdots$ & $G_n(t)$ & $\cdots$ & $G_n(T)$ \\ \hline
        $A(1)$   & $A(2)$   & $\cdots$ & $A(t)$   & $\cdots$ & $A(T)$   \\ \hline
        $X(1)$   & $X(2)$   & $\cdots$ & $X(t)$   & $\cdots$ & $X(T)$   \\ \hline
    \end{tabular}
    \caption{Expert learning}
    \label{tab:experts}
\end{table}

\begin{obs}
    We don't know a priori which is the best expert, but we want to make predictions at least as good as theirs, so we need to understand which is the best expert while the times goes on.
\end{obs}

We say that expert $i$ ends up making $m_i$ mistakes if $\abs{\{ t \st t \in [T] \wedge G_i(t) \neq X(t) \}} = m_i$.\\
We call $m^*$ the number of mistakes made by the best expert, i.e., $m^* := \min_{i \in [n]} m_i$.

\section{Perfect expert}\label{sec:perfect-expert}

Suppose that we know that there exists an expert who makes no errors, that is, $\exists\ i^* \in [n] \st m_i^* = 0$. We want to find an algorithm to make as few errors as possible.

We present an algorithm that tries to achieve this result (but actually fails):
\begin{lstlisting}[caption={Algo 0}, label={lst:exp-algo0}]
algo0:
    $S \gets [n]$   // trusted experts
    for each $t \geq 1$:
        pick any $i \in S$
        declare $A(t) := G_i(t)$
        // $X$ is revealed, phase 1 ends and phase 2 begins
        $S \gets S - \{i \st G_i(t) \neq X(t)\}$
\end{lstlisting}

\begin{lem}\label{lem:exp-algo0-errors}
    \texttt{Algo0} makes at most $n-1$ mistakes.
\end{lem}
\begin{proof}
    Let $T_e = \{t_j \st A(t_j) \neq X(t_j)\}$ be the times in which the algorithm makes a mistake.\\
    Then, $\exists\ i \in [n]$ (an expert) such that:
    \begin{enumerate}
        \item $G_i(t_j) \neq X(t_j)$, and
        \item $\forall\ t < t_j,\ G_i(t) = X(t)$.
    \end{enumerate}
    Therefore, $\abs{T_e} \leq n-1$.
\end{proof}

\begin{lem}\label{lem:exp-algo0-tight}
    The bound given by lemma [\ref{lem:exp-algo0-errors}] is tight.
\end{lem}
\begin{proof}
    We give a counterexample to prove the lemma:
    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|c|c|c|l}
            \hline
            \multirow{5}{*}{experts} & \textit{\textbf{0}} & 1                   & 1                   & 1                   & 1                   & \multicolumn{1}{l|}{\xmark}     \\ \cline{2-7} 
            & 1                   & \textit{\textbf{0}} & 1                   & 1                   & 1                   & \multicolumn{1}{l|}{\xmark}     \\ \cline{2-7} 
            & 1                   & 1                   & \textit{\textbf{0}} & 1                   & 1                   & \multicolumn{1}{l|}{\xmark}     \\ \cline{2-7} 
            & 1                   & 1                   & 1                   & \textit{\textbf{0}} & 1                   & \multicolumn{1}{l|}{\xmark}     \\ \cline{2-7} 
            & 1                   & 1                   & 1                   & 1                   & \textit{\textbf{1}} & \multicolumn{1}{l|}{\cmark} \\ \hline
            algorithm                & 0                   & 0                   & 0                   & 0                   & 1                   &                                 \\ \cline{1-6}
            truth                    & 1                   & 1                   & 1                   & 1                   & 1                   &                                 \\ \cline{1-6}
        \end{tabular}
        \caption{Counterexample for proving lemma [\ref{lem:exp-algo0-errors}]'s tightness}
        \label{tab:exp-alg0-tight}
    \end{table}

    Note that the values in bold and italic corresponds to the guesses picked and copied by the algorithm, while the symbols \cmark and \xmark\ correspond to right or wrong guesses made by the algorithm, respectively.
    
    At each time $t$, a different expert makes a mistake, and the algorithm picks exactly that expert, so that it makes a mistake until the last step, when it finally find the ``perfect expert''.
\end{proof}

Now we present another algorithm that obtains better results:
\begin{lstlisting}[caption={Algo 1}, label={lst:exp-algo1}]
algo1:
    $S \gets [n]$   // trusted experts
    for each $t \geq 1$:
        $S^1 \gets \{i | i \in S \wedge G_i(t)=1\}$
        $S^0 \gets \{i | i \in S \wedge G_i(t)=0\}$
        if $\abs{S^1} > \abs{S^0}$:
            declare $A(t) := 1$
        else:
            declare $A(t) := 0$
        // $X$ is revealed, phase 1 ends and phase 2 begins
        $S \gets S - \{i \st G_i(t) \neq X(t)\}$
\end{lstlisting}

\obs With this algorithm, at each time $t$ we don't look at a single guess, but at all the guesses at that time.

\begin{lem}\label{lem:exp-alg1-1}
    At each time $t_j \st A(t_j) \neq X(t_j)$, that is, each time the algorithm makes a mistake, $S$ gets reduced by a factor $\geq 2$.
\end{lem}
\begin{proof}
    If the algorithm makes a mistake, if means that at least $n/2$ experts made a mistake, and they will be removed from $S$.
\end{proof}

\begin{cor}\label{cor:exp-alg1-1}
    $j \leq \log_2(n)$ for each $t_j \st A(t_j) \neq X(t_j),\ t_1 < t_2 < \ldots < t_j < \ldots$.
\end{cor}

\begin{obs}
    We can look at the predictions made by the experts as all the possible binary strings, so our problem becomes that of guess a binary string with length $\log(n)$ bits, so we need at least $\log(n)$ steps, since the string is random, thus the bound expressed in corollary [\ref{cor:exp-alg1-1}] is tight for each possible algorithm, and so \texttt{algo1} is optimal.
    
    \begin{table}[h]
        \centering
        \begin{tabular}{c|c|c|c}
            & $t_1$ & $t_2$ & $t_3$ \\ \hline
            $E_0$ & 0     & 0     & 0     \\
            $E_1$ & 0     & 0     & 1     \\
            $E_2$ & 0     & 1     & 0     \\
            $E_3$ & 0     & 1     & 1     \\
            $E_4$ & 1     & 0     & 0     \\
            $E_5$ & 1     & 0     & 1     \\
            $E_6$ & 1     & 1     & 0     \\
            $E_7$ & 1     & 1     & 1    
        \end{tabular}
        \caption{The experts' guesses seen as random binary strings.}
        \label{tab:exp-alg1-strings}
    \end{table}
\end{obs}


\section{Imperfect experts}\label{sec:imperfect-experts}

Now we drop the hypothesis that there is an expert that doesn't make mistakes.

First of all, note that, if $m^* > 0$, \texttt{algo1} [\ref{lst:exp-algo1}] doesn't give us any guarantee about the number of mistakes it will do, since we could throw away all the experts at the first step, for example, so we want some way to reinsert the experts who made some mistakes in the set of trusted experts.

We have two strategies to do that:
\begin{enumerate}
    \item When all the experts have done at least one mistake, we put all of them in the set again. In this way we are splitting the total time in batches: we'll have at most $m^*$ batches, and we'll do at most $\log_2(n)$ mistakes in each batch. See section [\ref{sec:exp-first}].\label{exp-first}
    \item We associate a rational number to each expert, to indicate how much we trust that expert, in this way we'll obtain at most $(2 + \varepsilon)\cdot(m^* + 1)$ mistakes. See section [\ref{sec:exp-weighted-maj}].\label{exp-second}
\end{enumerate}


\subsection{First strategy}\label{sec:exp-first}
Now we describe and analyze the algorithm that applies the first strategy [\ref{exp-first}].
\begin{lstlisting}[caption={Algo 2}, label={lst:exp-algo2}]
algo2:
    $S \gets [n]$   // trusted experts
    for each $t \geq 1$:
        $S^x \gets \{i | i \in S \wedge G_i(t)=x\},\ (x \in \{0,1\})$
        if $\abs{S^1} > \abs{S^0}$:
            declare $A(t) := 1$
        else:
            declare $A(t) := 0$
        // $X$ is revealed, phase 1 ends and phase 2 begins
        $S \gets S - \{i \st G_i(t) \neq X(t)\}$
        if $S == \emptyset$:
            $S \gets [n]$   // reset $S$
\end{lstlisting}

\begin{lem}\label{lem:exp-algo2-errors}
    Let $m^*$ be the number of mistakes of the best expert, then \texttt{algo2} makes at most\\
    $(m^*+1)\cdot(\log_2(n+1))$ mistakes.
\end{lem}
\begin{proof}
    Let $r_1, r_2, \ldots, r_k$ be the times at which we reset.\\
    Observe that, from time 1 to $r_1-1$, \texttt{algo2} behaves exactly like \texttt{algo1}, then, the number of mistakes from 1 to $r_1-1$ is $\leq \log_2(n)$.
    At time $r_1$, \texttt{algo2} makes at most 1 mistake.\\
    From time $r_j$ ti time $r_{j+1}-1$, \texttt{algo2} will make at most $\log_2(n)$ mistakes, and at time $r_{j+1}$ it will make at most 1 extra mistake.\\
    Thus, the total number of mistakes is at most $(\log_2(n+1))\cdot(k+1)$.\\
    Since each expert has to make at least 1 mistake per phase, and since $\exists\ i^* \in [n]$ that makes $m^*$ mistakes, $k \leq m^*$.
\end{proof}


\subsection{Second strategy -- weighted majority} \label{sec:exp-weighted-maj}

The second strategy, introduced in [\ref{exp-second}], is based on the idea that it is always better to keep old information, rather than trow it away. So we give an algorithm that apply this principle:

\begin{lstlisting}[caption={Weighted Majority}, label={lst:exp-wm}]
WM:
    $w_i^0 \gets 1 \forall\ i \in [n]$   // trustworthiness weights
    for each $t \geq 1$:
        $s^x \gets \sum_{\substack{i \in [n]\\G_i(t)=x}} w_i^{t-1},\ (x \in \{0,1\})$
        if $s^1 > s^0$:
            declare $A(t) := 1$
        else:
            declare $A(t) := 0$
        // $X$ is revealed, phase 1 ends and phase 2 begins
        for each $i \in [n]$:
            $w_i^t = 
            \begin{cases}
                w_i^{t+1} &\text{if } G_i(t) = X(t)\\
                w_i^{(t-1)/2} &\text{otherwise}
            \end{cases}$
\end{lstlisting}

\begin{lem}\label{lem:exp-lm-errors}
    \texttt{WM} makes at most $2.4 \cdot(m^* + \log_2(n))$ mistakes.
\end{lem}

\obs This algorithm is better than [\ref{lst:exp-algo2}] since we have a sum between $m^*$ and $\log_2(n)$ instead of a multiplication.

\begin{proof}
    We'll analyze the algorithm using a \textit{potential}, that is a function of the current state that changes in time: our potential is
    \begin{equation}\label{eq:exp-wm-potential}
        W^t = \sum_{i=1}^{n} w_i^t
    \end{equation}
    
    \obs This potential is monotone since it always decreases with the increasing of $t$.
    
    As usual, we will proof the lemma going through some claims.
\end{proof}






